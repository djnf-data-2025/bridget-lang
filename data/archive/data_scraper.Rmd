---
title: "Richard Moley Article Scraping"
author: "Bridget Lang"
date: "2024-11-16"
output: html_document
---
```{r}
library(tidyverse)
library(pdftools)
library(dplyr)
library(rio)
library(tidytext)
library(quanteda)
library(knitr)
#install.packages("pdftools")
```

#Make a dataframe of all the names of the non-ai files 

```{r}
#from: https://stackoverflow.com/a/68032558
getMatch = function(rexp, str) regmatches(str, regexpr(rexp, str))

not_ai_index <- list.files("./moley_newsweek/not_ai", pattern="*txt") %>% 
  as.data.frame() |>
  rename(filename = 1) |> 
  mutate(index = row_number()) 

#get date from file name
not_ai_index <- not_ai_index %>%
  mutate(date = getMatch("[0-9]{4}-[0-9]{2}-[0-9]{2}",filename))

#extract year into own column
not_ai_index <- not_ai_index %>%
  mutate(year = getMatch("[0-9]{4}", date))
  
not_ai_index <- not_ai_index %>%
  mutate(year = as.integer(year)) 

#extract month into own column
not_ai_index <- not_ai_index %>%
  mutate(month = str_replace_all(getMatch("-[0-9]{2}-", date), "-", ""))

not_ai_index <- not_ai_index %>%
  mutate(month = as.integer(month)) 

#extract day into own column
not_ai_index <- not_ai_index %>%
  mutate(day = str_replace_all(str_replace_all(getMatch("-[0-9]{2}_", filename), "-", ""), "_", ""))

not_ai_index <- not_ai_index %>%
  mutate(day = as.integer(day)) 

not_ai_index <- not_ai_index %>%
  select(index, filename, date, year, month, day)

write.csv(not_ai_index, "not_ai_index.csv")
```
#Make a dataframe of all the names of the ai files 

```{r}
ai_index <- read.csv("./extracted_AI_moley_index_nov_20.csv")

ai_index_temp <- ai_index %>%
  select(filename, date2, Year, month_num, day)

ai_index_temp <- 
  rename(ai_index_temp, date = date2, year = Year, month = month_num)

ai_index <- ai_index_temp

write.csv(ai_index_temp, "ai_index.csv")

```

# Combine AI and Non-AI indexes

```{r}
article_index <- not_ai_index %>%
  bind_rows(ai_index_temp)

write.csv(ai_index_temp, "article_index.csv")

```

# Compiling text
```{r}
#Compiling text for moley_newsweek
compile_not_ai <- function(filename) {
    articles_df_temp <-read_lines(glue::glue("./moley_newsweek/not_ai/{filename}"))%>%
    as_tibble() %>%
    mutate(filename = filename)
  # Bind results to master articles_df
  # <<- returns to global environment
    not_ai_articles_df <<- not_ai_articles_df %>%
      bind_rows(articles_df_temp)
}

compile_ai <- function(filename) {
  
    articles_df_temp_2 <- read_lines(glue::glue("./moley_newsweek/ai/{filename}"))%>%
    as_tibble() %>%
    mutate(filename = filename)
  
  # Bind results to master articles_df
  # <<- returns to global environment
    ai_articles_df <<- ai_articles_df %>%
      bind_rows(articles_df_temp_2)
}

###
# Create elements needed to run function
###

# Create empty tibbles to store results
not_ai_articles_df <- tibble()

ai_articles_df <- tibble()

sapply(not_ai_index$filename, compile_not_ai)

sapply(ai_index$filename, compile_ai)


```
# Clean up not ai articles and get lines into dataframe 
```{r}

not_ai_articles_df$value <- iconv(not_ai_articles_df$value, from = "", to = "UTF-8", sub = "")  # Replace invalid characters with ""

# Additional step to remove non-printable and problematic characters
not_ai_articles_df$value <- gsub("[^[:print:]]+", " ", not_ai_articles_df$value)  # Remove non-printable characters
not_ai_articles_df$value <- gsub("\\s+", " ", trimws(not_ai_articles_df$value))  # Normalize whitespace

# Function to split text into chunks of 12 words
split_into_chunks <- function(text, n = 12) {
  words <- unlist(strsplit(text, "\\s+"))  # Split text into words
  split_words <- split(words, ceiling(seq_along(words) / n))  # Group words into chunks
  chunks <- sapply(split_words, paste, collapse = " ")  # Combine words back into strings
  return(chunks)
}

# Process the dataframe
library(dplyr)

not_ai_lines <- not_ai_articles_df %>%
  rowwise() %>%
  mutate(
    text_chunks = list(split_into_chunks(value))
  ) %>%
  unnest(text_chunks) %>%
  select(filename, text_chunks) %>%
  rename(sentence = text_chunks)

not_ai_lines <- not_ai_lines %>%
  inner_join(not_ai_index, c("filename")) 
not_ai_lines <- not_ai_lines %>%
  select(sentence, date, year, month, day, filename)
# get word counts
not_ai_word_counts <- not_ai_lines
not_ai_word_counts <- not_ai_word_counts %>%
    mutate(word_count = str_count(sentence, "\\S+")) %>% # Count words in each sentence
    group_by(filename) %>%
    summarise(total_word_count = sum(word_count, na.rm = TRUE))

not_ai_index <- not_ai_index %>%
  inner_join(not_ai_word_counts, "filename")


```

# Clean up ai articles and get lines into dataframe 
```{r}

ai_articles_df <- rename(ai_articles_df, sentence = value)

ai_articles_df$sentence <- iconv(ai_articles_df$sentence, from = "", to = "UTF-8", sub = "")  # Replace invalid characters with ""

# Additional step to remove non-printable and problematic characters
ai_articles_df$sentence <- gsub("[^[:print:]]+", " ", ai_articles_df$sentence)  # Remove non-printable characters
ai_articles_df$sentence <- gsub("\\s+", " ", trimws(ai_articles_df$sentence))  # Normalize whitespace

#remove blank lines 
ai_articles_df <- ai_articles_df %>%
  
  filter(trimws(sentence) != "") 


# Process the dataframe
library(dplyr)

ai_lines <- ai_articles_df %>%
  select(filename, sentence)

ai_lines <- ai_lines %>%
  inner_join(ai_index, c("filename")) 

ai_lines

ai_lines <- ai_lines %>%
  select(sentence, date, year, month, day, filename)
# Print the result

ai_word_counts <- ai_lines
ai_word_counts <- ai_word_counts %>%
    mutate(word_count = str_count(sentence, "\\S+")) %>% # Count words in each sentence
    group_by(filename) %>%
    summarise(total_word_count = sum(word_count, na.rm = TRUE))

ai_index <- ai_index %>%
  inner_join(ai_word_counts, c("filename"))

article_index <- NULL

article_index <- ai_index %>%
  bind_rows(not_ai_index)
  
write.csv(article_index,"./article_index.csv")


```
# Merge text into one large dataframe 
```{r}
articles_lines <- not_ai_lines %>%
  bind_rows(ai_lines)

write.csv(articles_lines,"./articles_lines.csv")

```

# Make a dataframe with one word per row
```{r}
article_text <-  read.csv("./articles_lines.csv")

data(stop_words)
one_word_per_row <- article_text %>% mutate(sentence= str_squish(sentence)) |> 
  mutate(text = tolower(sentence)) |>  
  mutate(text = gsub("\\d+", "", text)) |>
  mutate(text = str_replace_all(text, "- ", "")) %>% 
  unnest_tokens(word, text, token="ngrams", n=1 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!is.na(word))
one_word_per_row

write_csv(one_word_per_row, "./one_word_per_row.csv")
```
# Create bigrams and clean them
```{r}
bigrams <- article_text %>% mutate(sentence= str_squish(sentence)) |> 
  mutate(text = tolower(sentence)) |>  
  mutate(text = gsub("\\d+", "", text)) |>
  mutate(text = str_replace_all(text, "raymond", "")) %>% 
  mutate(text = str_replace_all(text, "newsweek", "")) %>% 
  mutate(text = str_replace_all(text, "image", "")) %>%
  mutate(text = str_replace_all(text, "perspective", "")) %>%
  mutate(text = str_replace_all(text, "- ", "")) %>%
  mutate(text = str_replace_all(text, " -", "")) %>%
  mutate(text = str_replace_all(text, " - ", "")) %>%
  unnest_tokens(word, text, token="ngrams", n=2 ) %>%
  filter(!word %in% stop_words$word) %>%
  filter(!word == "minor inaccuracies") %>%
  filter(!word == "text extraction") %>%
  filter(!word == "text version") %>%
  filter(!word == "patent office") %>%
  filter(!word == "u.s. patent") %>%
  filter(!word == "u.s patent") %>%
  filter(!word == "provided note") %>%
  filter(!word == "column layout") %>%

  filter(!word == "registered u.s") %>%

  filter(!is.na(word))

bigrams <- bigrams %>%
  select(word, date, year, month, day, filename)

bigrams
```

# Cleaning bigrams
```{r}
bigrams_separated <- bigrams %>%
  separate(word, c("word1", "word2"), sep = " ")

#bigrams with stop words filtered

bigrams_filtered <- 
  bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

write.csv(bigrams_filtered, "bigrams_filtered.csv")

bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE) %>% 
  filter(!is.na(word1))

bigram_counts

write.csv(bigram_counts, "bigram_counts.csv")

  
top_20_bigrams <- bigram_counts |> 
  head(20) |> 
  mutate(bigram = paste(word1, " ", word2)) |> 
  select(bigram, n)

top_20_bigrams
write.csv(top_20_bigrams, "top_20_bigrams.csv")

```